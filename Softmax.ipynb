{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax  \n",
    "$ h = WX + b $  \n",
    "$ p_i = {\\exp(h_i)\\over\\sum{\\exp(h_i)}} $  \n",
    "$ L = -\\sum{T_i\\log(p_i)} $  \n",
    "$ {\\partial L\\over\\partial h_i} = p_i - T_i $  \n",
    "$ {\\partial h_i\\over\\partial W_i} = X $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ {\\partial L\\over\\partial h_i}$ 설명    \n",
    "<img src=\"img/fig a-5.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_cifar_10 import *\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.params = {}\n",
    "        self.params['W'] = 0.0001 * np.random.randn(3072, 10)\n",
    "        self.params['b'] = np.ones(10)\n",
    "    def forward(self, X):\n",
    "        #Softmax 함수\n",
    "        W = self.params['W']\n",
    "        b = self.params['b']\n",
    "        #p = np.exp(np.dot(X, W) + b)\n",
    "        h = np.dot(X, W) + b\n",
    "        #stable a\n",
    "        a = np.exp(h - np.max(h, axis = 1).reshape(-1,1))\n",
    "        p = a/np.sum(a, axis = 1).reshape(-1,1)\n",
    "        return p\n",
    "    \n",
    "    def loss(self, X, T):\n",
    "        \n",
    "        p = self.forward(X)\n",
    "        \n",
    "        n = T.shape[0]\n",
    "        \n",
    "        log_likelihood = -np.log(p[range(n), T])\n",
    "        Loss = np.sum(log_likelihood) / n\n",
    "        #Loss는 데이터 개수 전부 더한거 아닌가?\n",
    "        #Loss = np.sum(log_likehood)\n",
    "        return Loss\n",
    "    \n",
    "    def accuracy(self, X, T):\n",
    "        p = self.forward(X) #예측\n",
    "        predict = np.argmax(p, axis = 1) #예측 결과 index 1darray 로 출력 \n",
    "        \n",
    "        return 1 - np.count_nonzero(predict - T)/len(T)\n",
    "        \n",
    "    def gradient(self, X, T, learning_rate = 0.0001):\n",
    "        \n",
    "        p = self.forward(X)\n",
    "        #T = np.array(T)\n",
    "        t = np.zeros((T.shape[0], np.max(T) + 1))\n",
    "        t[np.arange(T.shape[0]), T] = 1\n",
    "        #t는 인덱스 레이블 T를 One hot 벡터로 바꾼 것\n",
    "        \n",
    "        #목적함수에 대한 가중치 미분값을 담을 zero array 생성\n",
    "        grads = {}\n",
    "        grads['W'] = np.zeros((3072, 10))\n",
    "        grads['b'] = np.zeros(10)\n",
    "        #목적함수에 대한 가중치 미분값 합 구하기\n",
    "        grads['W'] = (1/len(T)) * np.dot(X.T, p-t)\n",
    "        grads['b'] = (1/len(T)) * np.sum(p-t, axis = 0)\n",
    "\n",
    "        self.params['W'] -= learning_rate * grads['W']\n",
    "        self.params['b'] -= learning_rate * grads['b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Processing_data(train, test):\n",
    "    #change dtype\n",
    "    train = np.array(train, dtype=np.float64)\n",
    "    test = np.array(test, dtype=np.float64)\n",
    "    \n",
    "    #Reshaping\n",
    "    train = np.reshape(train, (train.shape[0], -1))\n",
    "    test = np.reshape(test, (test.shape[0], -1))\n",
    "    \n",
    "    #Normalizing\n",
    "    mean_image = np.mean(train, axis = 0)\n",
    "    #print(train.dtype)\n",
    "    train -= mean_image\n",
    "    test -= mean_image\n",
    "    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar_10_dir = 'cifar-10-batches-py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_filenames, train_labels, test_data, test_filenames, test_labels, label_names = \\\n",
    "load_cifar_10_data(cifar_10_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = Processing_data(train_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 3072)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(50000,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(10000, 3072)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(10000,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape\n",
    "train_labels.shape\n",
    "test_data.shape\n",
    "test_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data[:20]\n",
    "train_labels = train_labels[:20]\n",
    "test_data = test_data[:10]\n",
    "test_labels = test_labels[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([b'leptodactylus_pentadactylus_s_000004.png',\n",
       "       b'camion_s_000148.png', b'tipper_truck_s_001250.png',\n",
       "       b'american_elk_s_001521.png', b'station_wagon_s_000293.png',\n",
       "       b'coupe_s_001735.png', b'cassowary_s_001300.png',\n",
       "       b'cow_pony_s_001168.png', b'sea_boat_s_001584.png',\n",
       "       b'tabby_s_001355.png'], dtype='|S40')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = Softmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15976\n",
      "28.213080503499956\n",
      "0.22799999999999998\n",
      "32.43722659612448\n",
      "0.23197999999999996\n",
      "31.042650906750453\n",
      "0.23141999999999996\n",
      "27.68210914175905\n",
      "0.22162000000000004\n",
      "36.63419428172896\n",
      "0.21955999999999998\n",
      "41.70288534450684\n",
      "0.26961999999999997\n",
      "26.68141851596112\n",
      "0.2388\n",
      "34.00342800448953\n",
      "0.23462000000000005\n",
      "31.040791766268374\n",
      "0.23997999999999997\n",
      "33.321209963051665\n"
     ]
    }
   ],
   "source": [
    "for i in range(50):\n",
    "    softmax.gradient(train_data, train_labels)\n",
    "    if i % 5 ==0:\n",
    "        print(\"Accuracy : \" , softmax.accuracy(train_data, train_labels))\n",
    "        print(\"Loss     : \" , softmax.loss(train_data, train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
