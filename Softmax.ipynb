{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax  \n",
    "$ h = WX + b $  \n",
    "$ p_i = {\\exp(h_i)\\over\\sum{\\exp(h_i)}} $  \n",
    "$ L = -\\sum{T_i\\log(p_i)} $  \n",
    "$ {\\partial L\\over\\partial h_i} = p_i - T_i $  \n",
    "$ {\\partial h_i\\over\\partial W_i} = X $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ {\\partial L\\over\\partial h_i}$ 설명    \n",
    "<img src=\"img/fig a-5.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_cifar_10 import *\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.params = {}\n",
    "        self.params['W'] = 0.0001 * np.random.randn(3072, 10)\n",
    "        self.params['b'] = np.ones(10)\n",
    "    def forward(self, X):\n",
    "        #Softmax 함수\n",
    "        W = self.params['W']\n",
    "        b = self.params['b']\n",
    "        #p = np.exp(np.dot(X, W) + b)\n",
    "        h = np.dot(X, W) + b\n",
    "        #stable a\n",
    "        a = np.exp(h - np.max(h, axis = 1).reshape(-1,1))\n",
    "        p = a/np.sum(a, axis = 1).reshape(-1,1)\n",
    "        return p\n",
    "    \n",
    "    def loss(self, X, T):\n",
    "        \n",
    "        p = self.forward(X)\n",
    "        \n",
    "        n = T.shape[0]\n",
    "        \n",
    "        log_likelihood = -np.log(p[range(n), T])\n",
    "        Loss = np.sum(log_likelihood) / n\n",
    "        #Loss는 데이터 개수 전부 더한거 아닌가?\n",
    "        #Loss = np.sum(log_likehood)\n",
    "        return Loss\n",
    "    \n",
    "    def accuracy(self, X, T):\n",
    "        p = self.forward(X) #예측\n",
    "        predict = np.argmax(p, axis = 1) #예측 결과 index 1darray 로 출력 \n",
    "        \n",
    "        return 1 - np.count_nonzero(predict - T)/len(T)\n",
    "        \n",
    "    def gradient(self, X, T, learning_rate = 0.0001):\n",
    "        \n",
    "        p = self.forward(X)\n",
    "        #T = np.array(T)\n",
    "        #t = np.zeros((T.shape[0], np.max(T) + 1)) np.max로 잡아주는게 좋으나 일부 배치에서는 인덱스 답이 작음\n",
    "        t = np.zeros((T.shape[0], 10))\n",
    "        t[np.arange(T.shape[0]), T] = 1\n",
    "        #t는 인덱스 레이블 T를 One hot 벡터로 바꾼 것\n",
    "        \n",
    "        #목적함수에 대한 가중치 미분값을 담을 zero array 생성\n",
    "        grads = {}\n",
    "        grads['W'] = np.zeros((3072, 10))\n",
    "        grads['b'] = np.zeros(10)\n",
    "        #목적함수에 대한 가중치 미분값 합 구하기\n",
    "        grads['W'] = (1/len(T)) * np.dot(X.T, p-t)\n",
    "        grads['b'] = (1/len(T)) * np.sum(p-t, axis = 0)\n",
    "\n",
    "        self.params['W'] -= learning_rate * grads['W']\n",
    "        self.params['b'] -= learning_rate * grads['b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Processing_data(train, test):\n",
    "    #change dtype\n",
    "    train = np.array(train, dtype=np.float64)\n",
    "    test = np.array(test, dtype=np.float64)\n",
    "    \n",
    "    #Reshaping\n",
    "    train = np.reshape(train, (train.shape[0], -1))\n",
    "    test = np.reshape(test, (test.shape[0], -1))\n",
    "    \n",
    "    #Normalizing\n",
    "    mean_image = np.mean(train, axis = 0)\n",
    "    #print(train.dtype)\n",
    "    train -= mean_image\n",
    "    test -= mean_image\n",
    "    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar_10_dir = 'cifar-10-batches-py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_filenames, train_labels, test_data, test_filenames, test_labels, label_names = \\\n",
    "load_cifar_10_data(cifar_10_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = Processing_data(train_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 3072)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(50000,)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(10000, 3072)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(10000,)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape\n",
    "train_labels.shape\n",
    "test_data.shape\n",
    "test_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = train_data[:20]\n",
    "# train_labels = train_labels[:20]\n",
    "# test_data = test_data[:10]\n",
    "# test_labels = test_labels[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = Softmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "iters_num = 1000\n",
    "batch_size = 64\n",
    "train_size = train_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 번째 반복중입니다.\n",
      "Accuracy :  0.31942000000000004\n",
      "Loss     :  24.550373434471144\n",
      "40 번째 반복중입니다.\n",
      "Accuracy :  0.31588000000000005\n",
      "Loss     :  26.362522091842845\n",
      "80 번째 반복중입니다.\n",
      "Accuracy :  0.3345\n",
      "Loss     :  23.17246412954829\n",
      "120 번째 반복중입니다.\n",
      "Accuracy :  0.30800000000000005\n",
      "Loss     :  27.512185051746727\n",
      "160 번째 반복중입니다.\n",
      "Accuracy :  0.37778\n",
      "Loss     :  18.664059915569055\n",
      "200 번째 반복중입니다.\n",
      "Accuracy :  0.34968\n",
      "Loss     :  27.567814797099274\n",
      "240 번째 반복중입니다.\n",
      "Accuracy :  0.34097999999999995\n",
      "Loss     :  25.306544815382185\n",
      "280 번째 반복중입니다.\n",
      "Accuracy :  0.33220000000000005\n",
      "Loss     :  27.214612810485136\n",
      "320 번째 반복중입니다.\n",
      "Accuracy :  0.32202\n",
      "Loss     :  22.210010248294807\n",
      "360 번째 반복중입니다.\n",
      "Accuracy :  0.36973999999999996\n",
      "Loss     :  23.98068845767166\n",
      "400 번째 반복중입니다.\n",
      "Accuracy :  0.33792\n",
      "Loss     :  29.673246869081197\n",
      "440 번째 반복중입니다.\n",
      "Accuracy :  0.38593999999999995\n",
      "Loss     :  21.789254451318826\n",
      "480 번째 반복중입니다.\n",
      "Accuracy :  0.34109999999999996\n",
      "Loss     :  24.32039561828143\n",
      "520 번째 반복중입니다.\n",
      "Accuracy :  0.3698\n",
      "Loss     :  22.133773907875014\n",
      "560 번째 반복중입니다.\n",
      "Accuracy :  0.36846\n",
      "Loss     :  21.491657783677507\n",
      "600 번째 반복중입니다.\n",
      "Accuracy :  0.34897999999999996\n",
      "Loss     :  25.002409776980713\n",
      "640 번째 반복중입니다.\n",
      "Accuracy :  0.32576000000000005\n",
      "Loss     :  38.75561220250769\n",
      "680 번째 반복중입니다.\n",
      "Accuracy :  0.37183999999999995\n",
      "Loss     :  22.192643468936634\n",
      "720 번째 반복중입니다.\n",
      "Accuracy :  0.37734\n",
      "Loss     :  25.983908253090977\n",
      "760 번째 반복중입니다.\n",
      "Accuracy :  0.35962000000000005\n",
      "Loss     :  22.851373609880238\n",
      "800 번째 반복중입니다.\n",
      "Accuracy :  0.35297999999999996\n",
      "Loss     :  24.6791997819589\n",
      "840 번째 반복중입니다.\n",
      "Accuracy :  0.36644\n",
      "Loss     :  25.808672938877148\n",
      "880 번째 반복중입니다.\n",
      "Accuracy :  0.35463999999999996\n",
      "Loss     :  26.1455098218174\n",
      "920 번째 반복중입니다.\n",
      "Accuracy :  0.37492000000000003\n",
      "Loss     :  21.536867629868734\n",
      "960 번째 반복중입니다.\n",
      "Accuracy :  0.36097999999999997\n",
      "Loss     :  28.699498425389287\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-127-590a2576d866>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[1;31m#배치 데이터\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mbatch_mask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0mx_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbatch_mask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m         \u001b[0mt_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbatch_mask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0msoftmax\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(iters_num):\n",
    "    for j in range(int(train_size/batch_size)):\n",
    "        #배치 데이터\n",
    "        batch_mask = np.random.choice(train_size, batch_size)\n",
    "        x_batch = train_data[batch_mask]\n",
    "        t_batch = train_labels[batch_mask]\n",
    "        softmax.gradient(x_batch, t_batch)\n",
    "    if i % 40 ==0:\n",
    "        print(i,\"번째 반복중입니다.\")\n",
    "        #Accuracy와 Loss는 전체 데이터를 대상으로 보겠음.\n",
    "        print(\"Accuracy : \" , softmax.accuracy(train_data, train_labels))\n",
    "        print(\"Loss     : \" , softmax.loss(train_data, train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15976\n",
      "28.213080503499956\n",
      "0.22799999999999998\n",
      "32.43722659612448\n",
      "0.23197999999999996\n",
      "31.042650906750453\n",
      "0.23141999999999996\n",
      "27.68210914175905\n",
      "0.22162000000000004\n",
      "36.63419428172896\n",
      "0.21955999999999998\n",
      "41.70288534450684\n",
      "0.26961999999999997\n",
      "26.68141851596112\n",
      "0.2388\n",
      "34.00342800448953\n",
      "0.23462000000000005\n",
      "31.040791766268374\n",
      "0.23997999999999997\n",
      "33.321209963051665\n"
     ]
    }
   ],
   "source": [
    "# for i in range(50):\n",
    "#     softmax.gradient(train_data, train_labels)\n",
    "#     if i % 5 ==0:\n",
    "#         print(\"Accuracy : \" , softmax.accuracy(train_data, train_labels))\n",
    "#         print(\"Loss     : \" , softmax.loss(train_data, train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
